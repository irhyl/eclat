{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Run full notebook logic via helper script (captures outputs) and hydrate kernel state\n",
    "import subprocess, sys, json\n",
    "from pathlib import Path\n",
    "print('Running notebook logic via scripts/run_notebook02.py')\n",
    "proc = subprocess.run([sys.executable, 'scripts/run_notebook02.py'], capture_output=True, text=True)\n",
    "print(proc.stdout)\n",
    "if proc.stderr:\n",
    "    print('--- STDERR ---', file=sys.stderr)\n",
    "    print(proc.stderr, file=sys.stderr)\n",
    "print('Script exit code:', proc.returncode)\n",
    "\n",
    "# Hydrate kernel state from produced artifacts so later cells can display results\n",
    "ROOT = Path('.')\n",
    "DATA_DIR = ROOT.joinpath('data','raw')\n",
    "ARTIFACTS = ROOT.joinpath('artifacts')\n",
    "FEATURES_DIR = ROOT.joinpath('features')\n",
    "DRY_RUN = True\n",
    "SEED = 42\n",
    "\n",
    "# helper definitions used by later cells\n",
    "import random, numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def write_artifact(name, obj):\n",
    "    p = ARTIFACTS.joinpath(name)\n",
    "    with p.open('w', encoding='utf8') as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "    print('Wrote', p)\n",
    "\n",
    "# load features/sample if available\n",
    "features = None\n",
    "profiles = None\n",
    "logs = None\n",
    "paths_file = ARTIFACTS.joinpath('features_paths.json')\n",
    "if paths_file.exists():\n",
    "    with paths_file.open('r', encoding='utf8') as f:\n",
    "        paths = json.load(f)\n",
    "    sample = Path(paths.get('features_sample', ''))\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if sample.exists():\n",
    "            features = pd.read_parquet(sample)\n",
    "            print('Loaded features sample:', sample)\n",
    "    except Exception as e:\n",
    "        print('Could not load features sample:', e)\n",
    "\n",
    "# also attempt to load raw CSVs into profiles/logs for display cells\n",
    "try:\n",
    "    import pandas as pd\n",
    "    fp_profiles = DATA_DIR.joinpath('financial_profiles_large.csv')\n",
    "    fp_logs = DATA_DIR.joinpath('user_intent_logs.csv')\n",
    "    nrows = 1000 if DRY_RUN else None\n",
    "    if fp_profiles.exists():\n",
    "        try:\n",
    "            profiles = pd.read_csv(fp_profiles, nrows=nrows)\n",
    "            print('Loaded profiles from', fp_profiles)\n",
    "        except Exception as e:\n",
    "            print('Error reading profiles CSV:', e)\n",
    "    if fp_logs.exists():\n",
    "        try:\n",
    "            logs = pd.read_csv(fp_logs, nrows=nrows)\n",
    "            print('Loaded logs from', fp_logs)\n",
    "        except Exception as e:\n",
    "            print('Error reading logs CSV:', e)\n",
    "    # fallbacks to avoid None in downstream display cells\n",
    "    if profiles is None:\n",
    "        profiles = pd.DataFrame()\n",
    "    if logs is None:\n",
    "        logs = pd.DataFrame()\n",
    "    print('Hydrated profiles/logs (fallbacks applied).')\n",
    "except Exception as e:\n",
    "    print('Error loading raw CSVs:', e)\n",
    "    import pandas as pd\n",
    "    profiles = pd.DataFrame()\n",
    "    logs = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a313a3",
   "metadata": {},
   "source": [
    "## Notebook 02: Data Preparation and Deterministic Feature Engineering\n",
    "\n",
    "**What this notebook does:**\n",
    "- Loads the synthetic raw CSVs produced by `prototype/generate_data.py`.\n",
    "- Runs deterministic sampling when `DRY_RUN=True` to allow quick reviews.\n",
    "- Performs data-quality checks, simple EDA, and derives canonical features used by models.\n",
    "- Saves canonical feature tables and deterministic train/test splits and records provenance in `artifacts/`.\n",
    "\n",
    "**How to run:**\n",
    "1. Ensure you have the synthetic CSVs under `data/raw/` (run `prototype/generate_data.py` if missing).\n",
    "2. Keep `DRY_RUN=True` for a quick pass (uses smaller nrows); set `DRY_RUN=False` to process full datasets.\n",
    "3. Run cells top-to-bottom. Outputs will be written to `features/` and `artifacts/`.\n",
    "\n",
    "**Expected outputs:**\n",
    "- `features/features_table.parquet` — canonical feature table (full or sampled depending on `DRY_RUN`).\n",
    "- `features/features_sample.parquet` — small sample for quick experiments.\n",
    "- `artifacts/train.parquet`, `artifacts/test.parquet` — deterministic splits.\n",
    "- `artifacts/features_manifest.json` — feature provenance and transformation notes.\n",
    "- `artifacts/dq_data_prep.json` — data quality findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78857a0c",
   "metadata": {},
   "source": [
    "### Introduction: Why this notebook\n",
    "\n",
    "Careful, deterministic data preparation is critical for reproducible experiments and fair comparisons between LLM-enhanced modules and structured baselines. This notebook defines the canonical feature table used by subsequent modeling notebooks, documents transformation rationale, and emits artifacts that serve as the canonical input for experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ac4cf",
   "metadata": {},
   "source": [
    "### Data overview: files and schema\n",
    "\n",
    "Primary raw files (generated by `prototype/generate_data.py`):\n",
    "- `data/raw/financial_profiles_large.csv` — synthetic customer financial profiles (default 10,000 rows).\n",
    "- `data/raw/user_intent_logs.csv` — synthetic interaction logs with utterance timestamps, pause intervals, and LLM-intent labels (sampled).\n",
    "\n",
    "We will use these files to build: `features/features_table.parquet` (canonical features) and saved train/test splits under `artifacts/`.\n",
    "\n",
    "Note: amounts are in INR; conversions or normalization will be made explicit in feature definitions.\n",
    "________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03fc5d",
   "metadata": {},
   "source": [
    "##### 1. Setup constants and imports (`DRY_RUN`, `SEED`, path variables). Ensures directories exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup constants, flags, imports\n",
    "DRY_RUN = True  # set to False to process full datasets\n",
    "SEED = 42\n",
    "from pathlib import Path\n",
    "import os, sys, json, hashlib, time, random\n",
    "import numpy as np, pandas as pd\n",
    "import logging\n",
    "# Determine project root robustly: prefer repository root when executing from `notebooks/` during nbconvert\n",
    "cwd = Path.cwd()\n",
    "ROOT = Path('.')\n",
    "# if data/raw exists relative to cwd, keep ROOT as cwd, otherwise try parent (common when kernel starts in notebooks/)\n",
    "if not cwd.joinpath('data','raw').exists():\n",
    "    parent = cwd.parent\n",
    "    if parent.joinpath('data','raw').exists():\n",
    "        ROOT = parent\n",
    "    else:\n",
    "        ROOT = cwd\n",
    "else:\n",
    "    ROOT = cwd\n",
    "\n",
    "DATA_DIR = ROOT.joinpath('data','raw')\n",
    "FEATURES_DIR = ROOT.joinpath('features')\n",
    "ARTIFACTS = ROOT.joinpath('artifacts')\n",
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "print(f'ROOT={ROOT.resolve()} (cwd={cwd}) DRY_RUN={DRY_RUN} SEED={SEED}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ecb75",
   "metadata": {},
   "source": [
    "##### 2. Reproducibility helpers (`set_seed`, `write_artifact`) and initial seed setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921aea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: quick schema and head\n",
    "# Guard: ensure `profiles` and `logs` exist in the kernel; load if missing\n",
    "try:\n",
    "    profiles\n",
    "    logs\n",
    "except NameError:\n",
    "    fp_profiles = DATA_DIR.joinpath('financial_profiles_large.csv')\n",
    "    fp_logs = DATA_DIR.joinpath('user_intent_logs.csv')\n",
    "    nrows = 1000 if DRY_RUN else None\n",
    "    profiles = pd.read_csv(fp_profiles, nrows=nrows)\n",
    "    logs = pd.read_csv(fp_logs, nrows=nrows)\n",
    "    print('Loaded profiles/logs in guard')\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "display(profiles.head())\n",
    "print('\\nProfile schema:')\n",
    "print(profiles.dtypes.to_string())\n",
    "\n",
    "display(logs.head())\n",
    "print('\\nLogs schema:')\n",
    "print(logs.dtypes.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f40f57",
   "metadata": {},
   "source": [
    "##### 3. Load raw CSVs (with `nrows` sampling when `DRY_RUN=True`). Raises helpful error if files missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ad4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load raw CSVs (sample when DRY_RUN)\n",
    "fp_profiles = DATA_DIR.joinpath('financial_profiles_large.csv')\n",
    "fp_logs = DATA_DIR.joinpath('user_intent_logs.csv')\n",
    "\n",
    "if not fp_profiles.exists() or not fp_logs.exists():\n",
    "    raise FileNotFoundError('Expected raw CSVs under data/raw. Run prototype/generate_data.py if missing.')\n",
    "\n",
    "nrows = 1000 if DRY_RUN else None\n",
    "profiles = pd.read_csv(fp_profiles, nrows=nrows)\n",
    "logs = pd.read_csv(fp_logs, nrows=nrows)\n",
    "print('profiles', profiles.shape, 'logs', logs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c29cee",
   "metadata": {},
   "source": [
    "##### 4. Show quick schema and head of `profiles` and `logs` so reviewers can inspect column names and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01988c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: quick schema and head\n",
    "# Guard: if earlier load cell did not run, load a sample here so display still works\n",
    "try:\n",
    "    profiles\n",
    "    logs\n",
    "except NameError:\n",
    "    fp_profiles = DATA_DIR.joinpath('financial_profiles_large.csv')\n",
    "    fp_logs = DATA_DIR.joinpath('user_intent_logs.csv')\n",
    "    nrows = 1000 if DRY_RUN else None\n",
    "    profiles = pd.read_csv(fp_profiles, nrows=nrows)\n",
    "    logs = pd.read_csv(fp_logs, nrows=nrows)\n",
    "    print('Loaded profiles/logs in display cell')\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "display(profiles.head())\n",
    "print('\\nProfile schema:')\n",
    "print(profiles.dtypes.to_string())\n",
    "\n",
    "display(logs.head())\n",
    "print('\\nLogs schema:')\n",
    "print(logs.dtypes.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ea112",
   "metadata": {},
   "source": [
    "##### 5. Data-quality checks: missing values, duplicates, negative amounts — writes `dq_data_prep.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780496ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data quality checks\n",
    "dq = {}\n",
    "dq['profiles_missing'] = profiles.isnull().sum().to_dict()\n",
    "dq['logs_missing'] = logs.isnull().sum().to_dict()\n",
    "dq['profiles_duplicates'] = int(profiles.duplicated().sum())\n",
    "dq['logs_duplicates'] = int(logs.duplicated().sum())\n",
    "# Check currency column presence and non-negative amounts\n",
    "for col in ['loan_amount_inr', 'annual_income']:\n",
    "    if col in profiles.columns:\n",
    "        dq[f'{col}_negatives'] = int((profiles[col] < 0).sum())\n",
    "\n",
    "write_artifact('dq_data_prep.json', dq)\n",
    "dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf29dc2",
   "metadata": {},
   "source": [
    "##### 6. Quick visual EDA (histogram of `loan_amount_inr`) guarded by plotting availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Quick visual EDA (guarded by DRY_RUN and plotting availability)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt, seaborn as sns\n",
    "    sns.set(style='whitegrid', palette='pastel')\n",
    "    if 'loan_amount_inr' in profiles.columns:\n",
    "        plt.figure(figsize=(6,3))\n",
    "        sns.histplot(profiles['loan_amount_inr'].clip(lower=0), bins=50)\n",
    "        plt.title('Loan amount (INR) distribution')\n",
    "        plt.tight_layout()\n",
    "        display(plt.gcf())\n",
    "except Exception as e:\n",
    "    print('Plotting skipped or failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9db7f8",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________\n",
    "### Feature engineering plan:\n",
    "\n",
    "Planned canonical features (each with short rationale):\n",
    "- `loan_amount_inr` (raw): absolute exposure; useful for PD and LGD proxies.\n",
    "- `log_loan_amount` (log-scale): reduces skew, improves model stability.\n",
    "- `debt_to_income`: captures balance-sheet stress; predictive for default propensity.\n",
    "- `credit_score`: normalized score (if synthetic, keep original but record distribution).\n",
    "- `pause_mean`, `pause_std`, `pause_max`: derived from `user_intent_logs.csv` inter-utterance intervals to capture hesitation dynamics (theory: longer pauses correlate with indecision).\n",
    "- `intent_label` (LLM output placeholder): categorical label from LLM intent detector; use embedding-based features in later notebooks.\n",
    "\n",
    "All features will be documented in `artifacts/features_manifest.json` with precise transformation code and provenance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1b2a2",
   "metadata": {},
   "source": [
    "##### 7. Feature engineering scaffold: core transforms (log loan, debt-to-income, pause aggregations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a691e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Feature engineering scaffold (implement minimal derived features)\n",
    "def engineer_features(profiles_df, logs_df):\n",
    "    df = profiles_df.copy()\n",
    "    # log-transform loan amount\n",
    "    if 'loan_amount_inr' in df.columns:\n",
    "        df['loan_amount_inr'] = df['loan_amount_inr'].clip(lower=0)\n",
    "        df['log_loan_amount'] = np.log1p(df['loan_amount_inr'])\n",
    "    # debt to income (guard zeros)\n",
    "    if set(['existing_debt','annual_income']).issubset(df.columns):\n",
    "        df['debt_to_income'] = df['existing_debt'] / (df['annual_income'].replace(0, np.nan))\n",
    "        df['debt_to_income'] = df['debt_to_income'].fillna(df['debt_to_income'].median())\n",
    "    # placeholder: aggregate pause stats from logs_df grouped by user_id\n",
    "    if 'user_id' in logs_df.columns and 'pause_interval' in logs_df.columns:\n",
    "        agg = logs_df.groupby('user_id')['pause_interval'].agg(['mean','std','max']).rename(columns={'mean':'pause_mean','std':'pause_std','max':'pause_max'})\n",
    "        df = df.merge(agg, how='left', left_on='user_id', right_index=True)\n",
    "        df[['pause_mean','pause_std','pause_max']] = df[['pause_mean','pause_std','pause_max']].fillna(0)\n",
    "    return df\n",
    "\n",
    "features = engineer_features(profiles, logs)\n",
    "print('features shape', features.shape)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b596948",
   "metadata": {},
   "source": [
    "##### 8. Save canonical features and a sampled parquet for fast downstream usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save canonical features and a small sample for quick experiments\n",
    "features_path = FEATURES_DIR.joinpath('features_table.parquet')\n",
    "features.to_parquet(features_path, index=False)\n",
    "print('Wrote', features_path)\n",
    "# small sample for notebooks when DRY_RUN=True\n",
    "sample_path = FEATURES_DIR.joinpath('features_sample.parquet')\n",
    "features.sample(min(1000, len(features)), random_state=SEED).to_parquet(sample_path, index=False)\n",
    "print('Wrote', sample_path)\n",
    "write_artifact('features_paths.json', {'features_table': str(features_path), 'features_sample': str(sample_path)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1563c59",
   "metadata": {},
   "source": [
    "##### 9. Deterministic train/test split and writing of `artifacts/splits.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train/test split (deterministic)\n",
    "from sklearn.model_selection import train_test_split\n",
    "label_col = 'target' if 'target' in features.columns else None\n",
    "if label_col is None:\n",
    "    # create a synthetic proxy target for experiments (e.g., high risk if credit_score < threshold)\n",
    "    features['target'] = (features.get('credit_score', 600) < 650).astype(int)\n",
    "    label_col = 'target'\n",
    "train, test = train_test_split(features, test_size=0.2, random_state=SEED, stratify=features[label_col] if features[label_col].nunique()>1 else None)\n",
    "train_path = ARTIFACTS.joinpath('train.parquet')\n",
    "test_path = ARTIFACTS.joinpath('test.parquet')\n",
    "train.to_parquet(train_path, index=False)\n",
    "test.to_parquet(test_path, index=False)\n",
    "write_artifact('splits.json', {'train': str(train_path), 'test': str(test_path)})\n",
    "print('Wrote train/test splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Write feature manifest (provenance)\n",
    "features_manifest = {\n",
    "    'created_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "    'seed': SEED,\n",
    "    'features': [\n",
    "        'loan_amount_inr', 'log_loan_amount', 'existing_debt', 'annual_income', 'debt_to_income',\n",
    "        'credit_score', 'pause_mean', 'pause_std', 'pause_max', 'target'\n",
    "    ]\n",
    "}\n",
    "write_artifact('features_manifest.json', features_manifest)\n",
    "features_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b85f0",
   "metadata": {},
   "source": [
    "### References and Theory\n",
    "\n",
    "This section contains succinct references and brief notes explaining why the listed theory informs the transformations used in this notebook.\n",
    "\n",
    "- Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow — practical recommendations on log transforms, feature scaling, and engineering for model stability.\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning — background on bias/variance, regularization, and why transformations (e.g., log) help some models.\n",
    "- Brier, G. W. (1950). Verification of forecasts expressed in terms of probability — motivates probabilistic forecast evaluation metrics used downstream.\n",
    "- Pineau, J., et al. (2018-2020). Improving Reproducibility in Machine Learning Research — guidance on fixing randomness, CI practices, and artifact recording.\n",
    "- Conversational pause/hesitation studies (overview): empirical work in psycholinguistics and HCI shows pauses often correlate with uncertainty or increased cognitive load; here we use summary statistics (mean/std/max) of pause intervals as an engineered signal. Representative readings: Brown & Perrault (2019) and survey chapters in conversational HCI literature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
