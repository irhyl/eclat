{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32a08e7",
   "metadata": {},
   "source": [
    "## Notebook 04: Sales Agent\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook provides an auditable, minimal‑complexity reference implementation for a production‑oriented sales assistant. Its purpose is not to be a full product, but to illustrate the core engineering and algorithmic primitives needed to build a trustworthy pipeline: deterministic scoring, explicit persona policies, verifiable conversation flows, reproducible offer generation, lightweight analytics, and a transparent surrogate for narrative resonance with an explicit fairness correction.\n",
    "\n",
    "### Why this design?\n",
    "\n",
    "- Reproducibility: deterministic transforms + append‑only event logs allow an experiment to be replayed precisely. This is essential for audits and regulatory scrutiny.\n",
    "- Interpretability: we prefer arithmetic, linear contributions, and exposed intermediate values so every decision can be decomposed into human‑readable parts (weights × features).\n",
    "- Safety: constrained personas and guarded fallbacks reduce the attack surface compared to unconstrained generative agents.\n",
    "\n",
    "### Scope and constraints\n",
    "\n",
    "This prototype intentionally keeps models tiny and decisions explicit. For example, a linear surrogate is used for narrative scores so per‑feature attributions are exact: if $f_i$ are features and $w_i$ are weights, then the surrogate score is $S=\\sum_i w_i f_i$, and each contribution is exactly $w_i f_i$. This exactness makes explanation and fairness adjustments straightforward.\n",
    "\n",
    "Artifacts written by the notebook (examples, audits, schema samples) are stored under the `artifacts/` folder for downstream validation and archival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5062ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and artifacts directory\n",
    "import json, math, random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "ARTIFACTS = Path('artifacts')\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "print('Environment ready. Artifacts dir:', ARTIFACTS)\n",
    "print('matplotlib backend:', plt.get_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2ecee",
   "metadata": {},
   "source": [
    "### 1. Persona: Tone, Claims, and Fallbacks\n",
    "\n",
    "**Short description:**\n",
    "\n",
    "A `persona` here is a small, deterministic policy that maps contextual state and system confidence to a constrained response style and a limited set of templated utterances. Rather than leaving style as an emergent property of a large model, we explicitly encode allowed claims and fallback strategies so we can verify safety properties and run unit tests.\n",
    "\n",
    "Let $S$ be the finite set of conversational states (e.g. `greeting`, `qualification`, `objection`), and let $C$ be a confidence score or set of flags representing epistemic uncertainty. The persona implements a deterministic mapping $\\pi:\\,S\\times C\\rightarrow A$, where $A$ is a small set of action templates (labelled utterance forms). For example, $\\pi(qualification, low) = \\text{escalate}$.\n",
    "\n",
    "Why this matters (safety and verifiability):\n",
    "\n",
    "- Constraining outputs reduces the possible incorrect claims an agent can make and therefore reduces risk.\n",
    "- Deterministic templates are trivially unit‑testable and can be formally verified against policy constraints (e.g. \"no pricing claims without evidence\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0345eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persona example: simple template mapping\n",
    "from dataclasses import dataclass, asdict\n",
    "@dataclass\n",
    "class Persona:\n",
    "    name: str\n",
    "    tone: str\n",
    "    templates: dict\n",
    "p = Persona(name='SalesPro', tone='consultative', templates={'greet': 'Hi {name}, thanks for reaching out.', 'followup': 'Are you available for a short call this week?'})\n",
    "print('Persona preview:')\n",
    "print(asdict(p))\n",
    "# render a template\n",
    "print('\\nRendered greeting ->', p.templates['greet'].format(name='Alex'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbb1d2",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Core Conversation Flows: Finite State Machines and Verification\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Conversation flows are represented as finite state machines (FSMs) with guarded transitions. This representation provides a compact, analyzable model for dialog control: states capture semantic phases (e.g. `qualify`, `proposal`), and transitions are triggered by well‑defined events or predicates over user/system signals.\n",
    "\n",
    "**Mathematical description:**\n",
    "\n",
    "An FSM is a tuple $(S,\\Sigma,\\delta,s_0,F)$ where $S$ is a finite set of states, $\\Sigma$ is the event alphabet, $\\delta:\\,S\\times\\Sigma\\rightarrow S$ is a deterministic transition function, $s_0$ is the start state, and $F$ a set of terminal states. In practice we encode guards on transitions as predicates $g: Context\\rightarrow \\{True,False\\}$ and perform transition only when $g$ holds and the event matches.\n",
    "\n",
    "**Why use FSMs:**\n",
    "\n",
    "- Predictability: FSMs limit unexpected branching compared to unconstrained dialog managers.\n",
    "- Verifiability: reachability, invariants, and absence of bad cycles can be checked systematically (e.g. ensure there's always an escape to a human `handoff` state).\n",
    "- Testability: deterministic transitions enable exhaustive unit tests for small flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aadf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple FSM flow and visualization\n",
    "G = nx.DiGraph()\n",
    "states = ['start','qualify','proposal','handoff','closed']\n",
    "G.add_nodes_from(states)\n",
    "G.add_edge('start','qualify')\n",
    "G.add_edge('qualify','proposal')\n",
    "G.add_edge('proposal','closed')\n",
    "G.add_edge('proposal','handoff')\n",
    "pos = nx.spring_layout(G, seed=2)\n",
    "plt.figure(figsize=(6,4))\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=1200, arrowsize=20)\n",
    "plt.title('Conversation Flow (simple)')\n",
    "plt.show()\n",
    "print('Flow nodes:', list(G.nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398062a1",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Lead Qualification: Deterministic, Interpretable Scoring\n",
    "\n",
    "**Intuition and goal:**\n",
    "\n",
    "Lead qualification compresses diverse signals (text, metadata, behavioral events) into a single prioritization score used for gating, routing, and ranking. In regulated or high‑stakes settings, we prefer interpretable, deterministic scoring functions because they permit exact attribution and straightforward audits.\n",
    "\n",
    "**Core model (linear surrogate):**\n",
    "\n",
    "We use a linear functional form: if $x_i$ are normalized features and $w_i$ are human‑audited weights then\n",
    "\n",
    "$$ {score} = \\sum_{i=1}^n w_i x_i.$$\n",
    "\n",
    "**Notes on calibration and probabilistic interpretation:**\n",
    "\n",
    "- If a probability interpretation is required, apply a monotone link such as the logistic sigmoid $\\sigma(z)=1/(1+e^{-z})$ to convert an unbounded linear score into $[0,1]$.\n",
    "- Weight selection can be manual (domain knowledge) or learned from labeled data using constrained regression with L1/L2 regularization to preserve sparsity and stability. Regularization helps avoid overfitting when the number of features is large compared to labeled examples.\n",
    "\n",
    "**Explainability:**\n",
    "\n",
    "A major advantage of the linear form is exact per‑feature attribution: the contribution of feature $i$ is exactly $w_i x_i$. This enables precise diagnostic statements (\"this lead scored 0.3 points because budget=1 × weight=0.3\"), and supports fair‑by‑construction auditing and remediation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c23870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple deterministic lead scorer\n",
    "def extract_features(text):\n",
    "    t = text.lower()\n",
    "    return {\n",
    "        'budget': 1 if 'budget' in t or '$' in t else 0,\n",
    "        'timeline': 1 if 'soon' in t or 'next' in t or 'month' in t else 0,\n",
    "        'decision_maker': 1 if 'i am' in t or 'we are' in t else 0\n",
    "    }\n",
    "def score_lead(text, weights=(0.5,0.3,0.2)):\n",
    "    f = extract_features(text)\n",
    "    vals = [f['budget'], f['timeline'], f['decision_maker']]\n",
    "    s = sum(w*v for w,v in zip(weights, vals))\n",
    "    return s, f\n",
    "examples = [ 'We have a $5000 budget and want to start next month', 'Curious about pricing', 'I am the manager and want a demo soon' ]\n",
    "scores = [score_lead(t) for t in examples]\n",
    "print('Lead scores and features:')\n",
    "for t,(s,f) in zip(examples,scores):\n",
    "    print('\\nTEXT:', t)\n",
    "    print('SCORE:', s, 'FEATURES:', f)\n",
    "# plot bar chart\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(range(len(scores)), [s for s,_ in scores], tick_label=[ 'ex1','ex2','ex3' ])\n",
    "plt.title('Lead Scores')\n",
    "plt.ylim(0,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f190571",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Objection Handling: Deterministic Response Policies\n",
    "\n",
    "**Principle:**\n",
    "\n",
    "Objection handling should be fast, safe, and reversible: provide concise corrective or clarifying responses while avoiding commitments that require human authorization. The mapping from detected objection class to a response is an explicit, deterministic lookup in this prototype, which simplifies verification and reduces risk.\n",
    "\n",
    "**Design pattern:**\n",
    "\n",
    "- Classify the user utterance into a small taxonomy of objections (price, timing, scope, etc.).\n",
    "- For each class, provide a brief templated response that either addresses the objection or escalates to a human if the objection implies a contractual or legal claim.\n",
    "\n",
    "**When to escalate**\n",
    "\n",
    "Escalate when the confidence in the classification is below a threshold, when the objection contains sensitive contract terms, or when the response would require privileged access (billing, refunds). Escalation preserves safety and creates a clear audit trail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objection mapping demo\n",
    "objections = {\n",
    "    'price': 'I understand pricing is important. Can I show value relevant to your goals?',\n",
    "    'timing': 'I hear timing is tight. Would a short trial or phased roll-out help?',\n",
    "    'needs': 'Thanks for sharing that. Can you tell me which features matter most?'\n",
    "}\n",
    "sample = ['price','needs','other']\n",
    "for s in sample:\n",
    "    resp = objections.get(s, 'I will connect you with a specialist for that request.')\n",
    "    print(f'Objection: {s} -> Response: {resp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c55c65",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Offer Generation and Pricing: Auditable Arithmetic\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Generate deterministic, auditable quotes from validated inputs so that every monetary claim can be traced to explicit arithmetic. This avoids ambiguous rounding errors and covert assumptions about taxes, fees, or bundling.\n",
    "\n",
    "**Canonical formula:**\n",
    "\n",
    "For quantity $q$, unit price $p$, and discount fraction $d$ the canonical computation is:\n",
    "\n",
    "$$\\text{subtotal} = q \\times p\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97406d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offer generation demo\n",
    "def quote(q, p, d=0.0):\n",
    "    subtotal = q*p\n",
    "    discount = subtotal * d\n",
    "    total = subtotal - discount\n",
    "    return {'qty':q,'unit':p,'subtotal':subtotal,'discount':discount,'total':total}\n",
    "q = 10; p = 199.0; d = 0.1\n",
    "qout = quote(q,p,d)\n",
    "print('Quote preview:', qout)\n",
    "# visualize breakdown\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar(['subtotal','discount','total'], [qout['subtotal'], qout['discount'], qout['total']], color=['#4c78a8','#f58518','#54a24b'])\n",
    "plt.title('Quote Breakdown')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5f0f9",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Analytics, Signals, and Metrics — Reproducible Event Streams\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "Treat the agent as a producer of an append‑only event stream: each user interaction, state transition, or decision emits a timestamped event. Append‑only logs are fundamental to reproducibility because they enable deterministic replay, offline analysis, and causal attribution in experiments.\n",
    "\n",
    "**Key metrics:**\n",
    "\n",
    "- Conversion rate: $CR=\\frac{\\text{number of closed deals}}{\\text{number of unique leads}}$.\n",
    "- Time‑to‑qualify: distribution summaries (median, 90th percentile).\n",
    "- Abandonment: fraction of leads that stop interacting before `qualified` state.\n",
    "\n",
    "**Statistical cautions:**\n",
    "\n",
    "- For small sample sizes, point estimates (like a single conversion rate) have high variance; use confidence intervals or Bayesian smoothing (e.g., Beta priors) before making operational decisions.\n",
    "- Instrumentation bias matters: make sure events are recorded consistently across channels and versions so comparisons are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate events and compute simple metrics\n",
    "events = [ {'lead_id':1,'event':'message'},{'lead_id':1,'event':'qualified'},{'lead_id':1,'event':'closed'}, {'lead_id':2,'event':'message'},{'lead_id':2,'event':'abandoned'} ]\n",
    "total = len({e['lead_id'] for e in events})\n",
    "closed = len([e for e in events if e['event']=='closed'])\n",
    "abandoned = len([e for e in events if e['event']=='abandoned'])\n",
    "conv = closed/total\n",
    "print(f'Total leads: {total}, closed: {closed}, abandoned: {abandoned}, conversion rate: {conv:.2f}')\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.bar(['closed','abandoned'], [closed,abandoned], color=['#2ca02c','#d62728'])\n",
    "plt.title('Outcomes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8712f6",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Integrations and Runtime: Adapters, Contracts, and Robustness\n",
    "\n",
    "**Principles**\n",
    "\n",
    "Adapters translate pure business logic artifacts into external system APIs (CRM, calendar, ticketing). Design them with the following properties:\n",
    "\n",
    "- Idempotency: repeated delivery of the same artifact should not create duplicate records. Use idempotency keys derived from the artifact identity.\n",
    "- Clear error semantics: classify transient vs permanent errors and only retry transient ones.\n",
    "- Thinness: keep business logic pure (no side effects) and place side‑effectful code in small, well‑tested adapter modules.\n",
    "\n",
    "**Contract testing:**\n",
    "\n",
    "Implement contract tests or schema validation to ensure that adapters produce payloads that external systems expect. Record adapter version information in artifacts to facilitate debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration stub: CRM payload builder\n",
    "def build_crm_payload(lead_id, name, score):\n",
    "    return {'lead_id':lead_id,'name':name,'score':score}\n",
    "payload = build_crm_payload(42,'ACME Corp',0.78)\n",
    "print('CRM payload (preview):')\n",
    "print(json.dumps(payload, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a31a4",
   "metadata": {},
   "source": [
    "---\n",
    "### 8. Deterministic Tests & Worked Examples\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Embed small, fast, deterministic assertions alongside examples so reviewers and CI can detect regressions early. These tests codify high‑level invariants (e.g., a lead with clear budget/timeline should score higher than a casual inquiry) rather than micro‑optimizing internal thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060236a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic tests\n",
    "assert score_lead('We have a $1000 budget and can start next month')[0] > 0.5\n",
    "assert score_lead('Just browsing')[0] < 0.5\n",
    "print('Deterministic tests passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb354c",
   "metadata": {},
   "source": [
    "---\n",
    "### 9. Artifacts & Schema Exports\n",
    "\n",
    "Rationale:\n",
    "\n",
    "Artifacts are the lingua franca between pipeline stages and external systems. Keep them compact, strongly typed, and accompanied by schema definitions (e.g., JSON Schema) so downstream consumers can validate inputs deterministically.\n",
    "\n",
    "Best practices followed:\n",
    "\n",
    "- Record schema version and generator version in each artifact.\n",
    "- Use integer timestamps and integer cents for amounts to avoid precision drift.\n",
    "- Provide a small JSON Schema for each artifact so adapters and tests can validate data before commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead = {'id':1,'name':'ACME','score':0.82}\n",
    "offer = {'id':1,'lead_id':1,'total':1791.0}\n",
    "with open(ARTIFACTS / 'lead_sample.json','w',encoding='utf-8') as f: json.dump(lead,f,indent=2)\n",
    "with open(ARTIFACTS / 'offer_sample.json','w',encoding='utf-8') as f: json.dump(offer,f,indent=2)\n",
    "print('Wrote artifacts:', ARTIFACTS / 'lead_sample.json', ARTIFACTS / 'offer_sample.json')\n",
    "print('\\nPreview lead:')\n",
    "print(json.dumps(lead,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906472d1",
   "metadata": {},
   "source": [
    "---\n",
    "### 10. Narrative Resonance (Prototype) & Fairness\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "We need a transparent quantity (resonance) that expresses how strongly a lead's language aligns with product narratives, together with an urgency score that captures immediacy. Because transparency is essential for auditing and remediation, we use a linear surrogate that yields exact per‑feature attributions and allows an analytically tractable fairness correction.\n",
    "\n",
    "**Model formulation:**\n",
    "\n",
    "Let features extracted from text be $f=(f_1,\\dots,f_m)$ and let two weight vectors $w^{(R)}$ and $w^{(U)}$ parameterize resonance and urgency respectively. The surrogate predictions are:\n",
    "\n",
    "$$R = {w^{(R)}}^\\top f, \\qquad U = {w^{(U)}}^\\top f.$$\n",
    "\n",
    "**Exact attribution:**\n",
    "\n",
    "Because the model is linear, the contribution of feature $i$ to $R$ is $w^{(R)}_i f_i$ (and similarly for $U$). These additive attributions make it trivial to produce human‑readable explanations and to audit the influence of any individual feature.\n",
    "\n",
    "**Fairness penalty derivation:**\n",
    "\n",
    "We employ a simple, defensible mitigation: when a creditworthiness proxy (e.g., `credit_score`) is below a threshold $c_0$, reduce the operational urgency by subtracting a fraction $\\lambda$ of the urgency's feature contributions. Concretely, if $U$ is raw urgency and $C_U=\\sum_i w^{(U)}_i f_i$ is the additive urgency contribution, the penalized urgency is:\n",
    "\n",
    "$$U_{penalized} = \\max\\{0,\\; U - \\lambda\\, C_U\\}\\quad\\text{when }\\text{credit}<c_0,$$\n",
    "\n",
    "**Interpretation and tradeoffs:**\n",
    "\n",
    "- The penalty reduces immediate pursuit when the financial signal suggests elevated counterparty risk.\n",
    "- Choosing $\\lambda$ and $c_0$ requires a policy decision balancing commercial opportunity against credit risk; treat these as tunable knobs and monitor downstream outcomes.\n",
    "\n",
    "**Auditability:**\n",
    "\n",
    "- Log raw and penalized values, feature attributions, and the reason code for any penalty application so retrospective analysis is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45991570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype linear 'NarrativeModel' with exact feature contributions\n",
    "from dataclasses import dataclass\n",
    "def text_features(text):\n",
    "    t = text.lower()\n",
    "    return {\n",
    "        'story_future': 1.0 if any(w in t for w in ('future','plan','start','next')) else 0.0,\n",
    "        'urgency': 1.0 if any(w in t for w in ('urgent','soon','priority')) else 0.0,\n",
    "        'emotional_valence': 1.0 if any(w in t for w in ('excited','love','great')) else 0.0,\n",
    "    }\n",
    "@dataclass\n",
    "class NarrativeModel:\n",
    "    w_resonance: dict = None\n",
    "    w_urgency: dict = None\n",
    "    def __post_init__(self):\n",
    "        if self.w_resonance is None:\n",
    "            self.w_resonance = {'story_future': 0.6, 'urgency': 0.1, 'emotional_valence': 0.3}\n",
    "        if self.w_urgency is None:\n",
    "            self.w_urgency = {'story_future': 0.0, 'urgency': 0.8, 'emotional_valence': 0.2}\n",
    "    def predict(self, text):\n",
    "        f = text_features(text)\n",
    "        contrib_res = {k: self.w_resonance.get(k,0.0)*f.get(k,0.0) for k in f}\n",
    "        contrib_urg = {k: self.w_urgency.get(k,0.0)*f.get(k,0.0) for k in f}\n",
    "        resonance = sum(contrib_res.values())\n",
    "        urgency = sum(contrib_urg.values())\n",
    "        return {'resonance': resonance, 'urgency': urgency, 'contrib_res': contrib_res, 'contrib_urg': contrib_urg}\n",
    "def apply_fairness_penalty(out, credit_score, c0=600, lamb=0.8):\n",
    "    raw_urgency = out['urgency']\n",
    "    contrib_urgency = sum(out['contrib_urg'].values())\n",
    "    if credit_score >= c0:\n",
    "        return {'raw_urgency': raw_urgency, 'penalized_urgency': raw_urgency, 'reason':'no penalty', 'contrib_urgency': contrib_urgency}\n",
    "    penalized = max(0.0, raw_urgency - lamb * contrib_urgency)\n",
    "    return {'raw_urgency': raw_urgency, 'penalized_urgency': penalized, 'reason': f'penalized (credit {credit_score} < {c0})', 'contrib_urgency': contrib_urgency}\n",
    "# Demo and compact audit\n",
    "model = NarrativeModel()\n",
    "examples = [\n",
    "    'I plan to start next month and am excited about the future',\n",
    "    'This is urgent, we need priority support',\n",
    "    'Curious about pricing, not urgent'\n",
    "]\n",
    "cases = [(examples[0], 700), (examples[1], 580), (examples[2], 620)]\n",
    "audit = []\n",
    "for text, credit in cases:\n",
    "    out = model.predict(text)\n",
    "    v = apply_fairness_penalty(out, credit)\n",
    "    rec = {'text': text, 'credit': credit, 'resonance': round(out['resonance'],3), 'raw_urgency': round(out['urgency'],3), 'penalized_urgency': round(v['penalized_urgency'],3), 'reason': v['reason']}\n",
    "    audit.append(rec)\n",
    "# write audit artifact and preview\n",
    "with open(ARTIFACTS / 'narrative_audit.json','w',encoding='utf-8') as f:\n",
    "    json.dump(audit,f,indent=2)\n",
    "import pprint\n",
    "pprint.pprint(audit)\n",
    "print('Wrote audit artifact ->', ARTIFACTS / 'narrative_audit.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd61a69",
   "metadata": {},
   "source": [
    "---\n",
    "### 11. LLM Integration: Fine‑tuning vs adapters, safety, and evaluation\n",
    "\n",
    "**Context:**\n",
    "\n",
    "Large language models (LLMs) offer expressive power beyond hand‑crafted linear surrogates, but they introduce complexity in auditability, cost, and safety. This cell supplies a guarded template for using LoRA adapters on a large causal model; it is intentionally non‑executable in ordinary environments to prevent accidental runs.\n",
    "\n",
    "**Design tradeoffs:**\n",
    "\n",
    "- Expressiveness vs transparency: a tuned LLM can capture subtler narrative patterns but will not yield exact per‑feature attributions unless additional explainability techniques are layered on.\n",
    "- Resource cost: fine‑tuning or even adapter application typically requires GPUs, substantial disk, and careful engineering to avoid data leakage.\n",
    "- Evaluation: any LLM‑based surrogate must be evaluated with the same audit metrics (resonance, fairness corrections, off‑policy risk sampling) and compared to the linear baseline before deployment.\n",
    "\n",
    "**Safety checklist before enabling:**\n",
    "\n",
    "- Access control and credential gating.\n",
    "- Red‑team style tests for hallucination and data‑leakage.\n",
    "- Post‑hoc explanation methods and conservative thresholds for automated action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14644f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM integration template (DO NOT RUN unless you have model access & GPU)\n",
    "if False:\n",
    "    # Example (requires internet, model access, and GPU):\n",
    "    # pip install transformers accelerate peft\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from peft import get_peft_model, LoraConfig\n",
    "    base_model = 'meta/llama-3.1-8b'  # placeholder: replace with actual repo/id you have access to\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model, device_map='auto')\n",
    "    # Apply LoRA config (example settings)\n",
    "    lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=['q_proj','v_proj'], lora_dropout=0.05)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    # Load adapter weights if available:\n",
    "    # model.load_state_dict(torch.load('path_to_lora_adapter.pt'))\n",
    "    # Example inference (token limits and prompt engineering required):\n",
    "    # inp = '...prompt about negotiation and narrative...'\n",
    "    # tok = tokenizer(inp, return_tensors='pt').to(model.device)\n",
    "    # out = model.generate(**tok, max_new_tokens=128)\n",
    "    # print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42b356",
   "metadata": {},
   "source": [
    "---\n",
    "### 12. Narrative Audit Visualization: Purpose and Interpretation\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Visualizing the relationship between resonance and penalized urgency provides a quick, human‑readable summary of how narrative alignment interacts with operational gating rules. Points in the upper‑right quadrant (high resonance, high penalized urgency) indicate strong matches that remain actionable after fairness adjustments; points where raw urgency is high but penalized urgency is low merit policy review.\n",
    "\n",
    "**How to read the plot:**\n",
    "\n",
    "- X axis: surrogate resonance score (interpretive alignment).\n",
    "- Y axis: penalized urgency (post‑mitigation).\n",
    "- Labels: include sample identifiers and the reason code for any penalty to make the visualization immediately actionable for analysts.\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "Use this visualization together with the stored audit records to iterate on $\\lambda$ and $c_0$ policy parameters and to identify false positives where the surrogate misattributes resonance due to spurious keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the narrative audit (resonance vs penalized urgency)\n",
    "import json\n",
    "from pathlib import Path\n",
    "p = Path('artifacts') / 'narrative_audit.json'\n",
    "if p.exists():\n",
    "    import matplotlib.pyplot as plt\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        audit = json.load(f)\n",
    "    res = [r['resonance'] for r in audit]\n",
    "    pen = [r['penalized_urgency'] for r in audit]\n",
    "    labels = [f'c{i+1}' for i in range(len(audit))]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(res, pen, s=60)\n",
    "    for i, (x, y) in enumerate(zip(res, pen)):\n",
    "        plt.text(x + 0.01, y + 0.01, labels[i])\n",
    "    plt.xlabel('Resonance')\n",
    "    plt.ylabel('Penalized urgency')\n",
    "    plt.title('Narrative resonance vs penalized urgency')\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Audit artifact not found:', p)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
